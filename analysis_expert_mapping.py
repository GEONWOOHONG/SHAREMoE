import os
import math
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import GPT2Config, GPT2LMHeadModel
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from tqdm import tqdm
import tiktoken
import warnings
from scipy.stats import entropy as shannon_entropy
from config import HF_DATASETS_CACHE, HASH_TABLE_PATH, CHECKPOINTS_DIR

from modeling import (
    convert_gpt2_to_moe,
    GPT2LayerMoE,
    HashRouter,
    Expert,
    Router,
    MoELayer,
)
from patches import (
    patch_model_basic,
    patch_model_for_hash_moe,
    patch_model_for_ours_com,
    block_moe_forward_patch,
)

from utils import set_seed, load_safetensors

warnings.filterwarnings("ignore")

import numpy as np

def _entropy_from_counts(cnt):
    p = cnt / (cnt.sum() + 1e-12)
    return float(-(p * np.log2(p + 1e-12)).sum())

def _mi_from_labels(x, y):
    x = x.astype(np.int64); y = y.astype(np.int64)
    Hx = _entropy_from_counts(np.bincount(x))
    Hy = _entropy_from_counts(np.bincount(y))
    base = int(max(x.max(), y.max())) + 1
    Hxy = _entropy_from_counts(np.bincount(x * base + y))
    return Hx + Hy - Hxy

def _gini(counts):
    x = np.sort(counts.astype(np.float64))
    if x.sum() <= 0: return 0.0
    n = len(x); cum = np.cumsum(x)
    return (n + 1 - 2 * (cum.sum() / cum[-1])) / n

def _ece_top1(probs, n_bins=15):
    bins = np.linspace(0, 1, n_bins + 1)
    idx = np.clip(np.digitize(probs, bins) - 1, 0, n_bins - 1)
    ece = 0.0
    for b in range(n_bins):
        mask = (idx == b)
        if not mask.any(): 
            continue
        conf = float(probs[mask].mean())
        acc  = 1.0
        ece += mask.mean() * abs(acc - conf)
    return float(ece)

def _brier_top1(probs):
    # top-1 ì‚¬ê±´ì˜ í™•ë¥  ì˜ˆì¸¡ì— ëŒ€í•œ Brier score proxy
    return float(((1.0 - probs) ** 2).mean())

# === Top-k invariant routing-confidence helpers ===
def _selected_set_metrics_from_scores(scores: torch.Tensor, k: int):
    """
    scores: [N, E] softmax ì§í›„ í™•ë¥  (ë˜ëŠ” normalizeëœ ì ìˆ˜)
    k: ì„ íƒ ì§‘í•© í¬ê¸° (Switch/top-1=1, GShard/top-2=2)
    Returns: dict of numpy arrays (cpu)
    """
    eps = 1e-12
    topk_vals, topk_idx = torch.topk(scores, k=k, dim=-1)           # [N,k]
    C = topk_vals.sum(dim=-1)                                       # [N]
    if k == 1:
        D = torch.ones_like(C)                                      # top-1ì´ë©´ ë‚´ë¶€ ë§ˆì§„=1
        H_sel = torch.zeros_like(C)
        EEC = torch.ones_like(C)
        S1 = torch.ones_like(C)                                     # ì§‘í•© ë‚´ ì ìœ ìœ¨=1
    else:
        pnorm = topk_vals / (C.unsqueeze(-1) + eps)                 # [N,k]
        # D: (p1 - p2) / sum; k>1 ê°€ì •
        D = (topk_vals[:, 0] - topk_vals[:, 1]) / (C + eps)
        H_sel = -(pnorm * (pnorm + eps).log()).sum(dim=-1) / math.log(k)
        EEC = 1.0 / (pnorm.pow(2).sum(dim=-1) + eps)
        S1 = topk_vals[:, 0] / (C + eps)

    return {
        "C": C.detach().float().cpu().numpy(),
        "D": D.detach().float().cpu().numpy(),
        "H_sel": H_sel.detach().float().cpu().numpy(),
        "EEC": EEC.detach().float().cpu().numpy(),
        "S1": S1.detach().float().cpu().numpy(),
        "topk_idx_cpu": topk_idx.detach().int().cpu().numpy(),      # specialization(top-2)ìš©
        "top1_idx_cpu": topk_idx[:, 0].detach().int().cpu().numpy()
    }


enc = tiktoken.get_encoding("gpt2")

set_seed(42)

def load_pile_validation_dataset():
    """Pile validation ë°ì´í„°ì…‹ ë¡œë“œ (HF í—ˆë¸Œì—ì„œ ìºì‹œ ì‚¬ìš©)"""
    from data import load_or_prepare_pile

    # HF ìºì‹œ ê²½ë¡œëŠ” config.HF_DATASETS_CACHEì— ì´ë¯¸ ë“¤ì–´ìˆìŒ
    train_ds, valid_ds = load_or_prepare_pile()
    print(f"âœ… Loaded validation split: {len(valid_ds)} samples")

    # meta.pile_set_nameì´ ë“¤ì–´ìˆëŠ”ì§€ í™•ì¸(í† í¬ë‚˜ì´ì¦ˆ ë²„ì „ì´ë©´ metaê°€ ê·¸ëŒ€ë¡œ ë“¤ì–´ìˆìŒ)
    if "meta" not in valid_ds.column_names:
        raise KeyError("âŒ validation split has no 'meta' column (pile_set_name required)")

    return valid_ds

@torch.no_grad()
def analyze_expert_specialization(
    model, dataloader, device, mode, pile_set_map,
    max_batches=None, run_specialization=True, run_confidence=True, run_routes=True
):
    """
    Optimized: 
    - GPUì—ì„œ argmax/max/topkê¹Œì§€ ì²˜ë¦¬ â†’ CPU ì „ì†¡ëŸ‰ ìµœì†Œí™”
    - route ì‹œí€€ìŠ¤ëŠ” ë°°ì¹˜ ë‚´ì—ì„œ np.uniqueë¡œ ë²¡í„°í™” ì§‘ê³„ í›„ ëˆ„ì 
    - MI ê³„ì‚°ì€ ë¼ìš°íŠ¸ í–‰ë ¬ë§Œ CPUë¡œ ë‚´ë¦° ë’¤ ë²¡í„°í™”
    """
    model.eval()

    # === Pre-cache MoE layers ===
    moe_layers = []
    for h in model.transformer.h:
        if isinstance(h.mlp, GPT2LayerMoE):
            moe_layers.append(h.mlp.moe)
    print(f"âœ… Found {len(moe_layers)} MoE layers")

    reverse_pile_set_map = {v: k for k, v in pile_set_map.items()}

    # accumulators
    specialization_stats = defaultdict(lambda: defaultdict(Counter))
    confidence_stats = defaultdict(lambda: {
        'selected_mass': [],     # C
        'decisiveness': [],      # D
        'h_sel': [],             # H_sel
        'eec': [],               # EEC
        'top1_share': [],        # S1
        'expert_choices': []     # top-1 ì„ íƒ eid (route/ë¹ˆë„ìš©)
    })
    # per-sourceëŠ” 'í™•ë¥ ' ëŒ€ì‹  Selected-Mass(C) í‰ê· ìœ¼ë¡œ ì „í™˜
    confidence_per_source_stats = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))  # [layer][expert][source] -> list of C

    domain_route_counter = defaultdict(Counter)  # domain -> Counter(route_id)  (ì••ì¶•í‚¤ ì‚¬ìš©)
    total_routes_per_domain = Counter()

    # fairness-safe metrics
    smoothness_vals = []
    interlayer_mi_by_dist = defaultdict(list)
    load_counts_acc = dict()
    calibration_rows = []
    loadbalance_rows = []

    # ë£¨í”„
    total_batches = min(len(dataloader), max_batches) if max_batches else len(dataloader)
    progress = tqdm(enumerate(dataloader), total=total_batches, desc=f"[{mode}] Analyzing Data")

    for i, batch in progress:
        if max_batches and i >= max_batches:
            break

        input_ids = batch["input_ids"].to(device, non_blocking=True)
        attn_mask = batch["attention_mask"].to(device, non_blocking=True).bool()
        pile_set_ids = batch["pile_set_id"]                 # CPU tensor
        B, S = input_ids.shape

        # GPUì—ì„œ í•œ ë²ˆì— ì‹¤í–‰
        with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
            _ = model(input_ids=input_ids, attention_mask=attn_mask)

        # GPU boolean mask (flatten)
        valid_t = attn_mask.view(-1)                         # [B*T] (cuda)
        # CPU ì¸¡ì—ì„œ ì“°ëŠ” ë¼ë²¨/ë„ë©”ì¸
        pile_set_ids_exp_cpu = pile_set_ids.unsqueeze(1).expand(B, S).reshape(-1).cpu().numpy()

        # ì´ ë°°ì¹˜ì—ì„œ ë ˆì´ì–´ë³„ argmax expert id (ìœ íš¨ í† í°ë§Œ) ëª¨ì•„ë‘ê¸°
        batch_layer_eids_valid = []

        for layer_idx, moe_layer in enumerate(moe_layers):
            scores = None
            eids_flat = None           # torch.long [B*T] on GPU

            if mode == "hash":
                # ì´ë¯¸ GPU long tensorë¡œ ê³„ì‚°ë¨
                eids_flat = moe_layer.hash_router.route(input_ids).view(-1)
            else:
                # ours_com / switch / gshard / expert_choice : last_scores ë˜ëŠ” router.last_scores
                if getattr(moe_layer, 'last_scores', None) is not None:
                    scores = moe_layer.last_scores            # [B*T, E] or [B,T,E] â†’ ì•„ë˜ì—ì„œ reshape
                elif hasattr(moe_layer, 'router') and getattr(moe_layer.router, 'last_scores', None) is not None:
                    scores = moe_layer.router.last_scores
                # shape ì •ê·œí™”
                if scores is not None and scores.dim() == 3:
                    scores = scores.view(-1, scores.size(-1))  # [B*T, E]

            # --- ê³µì •ì§€í‘œìš© top-k ì„¤ì • ---
            k_sel = 2 if mode == "gshard" else 1

            if scores is not None:
                with torch.no_grad():
                    row_sum = scores.sum(dim=-1, keepdim=True)
                need_norm = (scores.min() < 0) or (scores.max() > 1.0) or (
                    not torch.allclose(row_sum.mean(), torch.tensor(1.0, device=row_sum.device), atol=1e-3)
                )
                if need_norm:
                    scores = torch.softmax(scores.float(), dim=-1)

                met = _selected_set_metrics_from_scores(scores, k=k_sel)
                top1_gpu = torch.as_tensor(met["top1_idx_cpu"], device=scores.device, dtype=torch.long)  # í¸ì˜ìƒ gpuë¡œ
                eids_flat = top1_gpu
                if mode == "gshard":
                    topk_idx_cpu = met["topk_idx_cpu"][valid_t.cpu().numpy()]  # specializationì—ì„œ top-2 ì¹´ìš´íŠ¸ìš©

            if eids_flat is None:
                batch_layer_eids_valid.append(None)
                continue

            eids_valid_cpu = eids_flat[valid_t].int().cpu().numpy()              # [N_valid]
            batch_layer_eids_valid.append(eids_valid_cpu)

            # ---- Load-balance (per-layer counts) ëˆ„ì  (CPU ë²¡í„°í™”) ----
            E = int(scores.size(-1)) if scores is not None else int(eids_valid_cpu.max()) + 1
            if layer_idx not in load_counts_acc:
                load_counts_acc[layer_idx] = np.zeros(E, dtype=np.float64)
            elif load_counts_acc[layer_idx].shape[0] < E:
                tmp = np.zeros(E, dtype=np.float64)
                tmp[:load_counts_acc[layer_idx].shape[0]] = load_counts_acc[layer_idx]
                load_counts_acc[layer_idx] = tmp
            load_counts_acc[layer_idx][:E] += np.bincount(eids_valid_cpu, minlength=E).astype(np.float64)

            # ---- Specialization (ë„ë©”ì¸-ì „ë¬¸ê°€ ì¹´ìš´íŠ¸) ----
            if run_specialization:
                sids_valid = pile_set_ids_exp_cpu[valid_t.cpu().numpy()]   # [N_valid]
                if mode == "gshard" and scores is not None:
                    # top-2ë§Œ CPUë¡œ ì´ë¯¸ ë‚´ë ¤ì™€ ìˆìŒ
                    for (e1, e2), sid in zip(topk_idx_cpu, sids_valid):
                        src = reverse_pile_set_map[int(sid)]
                        specialization_stats[layer_idx][int(e1)][src] += 1
                        specialization_stats[layer_idx][int(e2)][src] += 1
                elif mode == "gshard" and scores is None:
                    for eid, sid in zip(eids_valid_cpu, sids_valid):
                        src = reverse_pile_set_map[int(sid)]
                        specialization_stats[layer_idx][int(eid)][src] += 1

            # ---- Confidence (Selected-Mass ê¸°ë°˜) ----
            if run_confidence and (scores is not None):
                vt = valid_t.cpu().numpy()
                C_valid  = met["C"][vt]
                D_valid  = met["D"][vt]
                H_valid  = met["H_sel"][vt]
                EEC_valid= met["EEC"][vt]
                S1_valid = met["S1"][vt]

                cs = confidence_stats[layer_idx]
                cs['selected_mass'].extend(C_valid.tolist())
                cs['decisiveness'].extend(D_valid.tolist())
                cs['h_sel'].extend(H_valid.tolist())
                cs['eec'].extend(EEC_valid.tolist())
                cs['top1_share'].extend(S1_valid.tolist())
                cs['expert_choices'].extend(eids_valid_cpu.tolist())

                # ë„ë©”ì¸ë³„ í‰ê· : ì´ì œ C(Selected-Mass)ë¥¼ ì €ì¥
                sids_valid = pile_set_ids_exp_cpu[vt]
                for c, eid, sid in zip(C_valid, eids_valid_cpu, sids_valid):
                    src = reverse_pile_set_map[int(sid)]
                    confidence_per_source_stats[layer_idx][int(eid)][src].append(float(c))

        # ---- Route ì‹œí€€ìŠ¤ (ë°°ì¹˜ ë‚´ ë²¡í„°í™”) ----
        if run_routes:
            # None ë ˆì´ì–´ ì œì™¸
            stacked = [e for e in batch_layer_eids_valid if e is not None]
            if stacked:
                routes_mat = np.stack(stacked, axis=0).astype(np.int16, copy=False)  # [L_eff, N_valid]
                routes_tok = routes_mat.T                                            # [N_valid, L_eff]

                # Smoothness: ì¸ì ‘ í† í°ê°„ í•´ë°ê±°ë¦¬ í‰ê· 
                if routes_tok.shape[0] > 1:
                    pair_diff = (routes_tok[1:] != routes_tok[:-1]).mean(axis=1)
                    smoothness_vals.append(float(1.0 - pair_diff.mean()))

                # Inter-layer MI: ë²¡í„°í™” ë£¨í”„(ì‘ì€ Lì´ë¼ ê°€ë³ì§€ë§Œ numpyë§Œ ì‚¬ìš©)
                L_eff = routes_tok.shape[1]
                for d in range(1, L_eff):
                    # (i, i+d) ìŒë§Œ ëª¨ì•„ í‰ê· 
                    mi_vals = []
                    for i0 in range(L_eff - d):
                        xi = routes_tok[:, i0]
                        xj = routes_tok[:, i0 + d]
                        # MI ê³„ì‚°(ë²¡í„°í™”ëœ ì¹´ìš´íŠ¸)
                        base = int(max(xi.max(), xj.max())) + 1
                        joint = np.bincount(xi.astype(np.int64) * base + xj.astype(np.int64))
                        joint = joint[joint > 0].astype(np.float64)
                        pxy = joint / joint.sum()
                        px = np.bincount(xi, minlength=base).astype(np.float64); px = px[px > 0]; px /= px.sum()
                        py = np.bincount(xj, minlength=base).astype(np.float64); py = py[py > 0]; py /= py.sum()
                        Hx = -(px * np.log(px)).sum(); Hy = -(py * np.log(py)).sum()
                        Hxy = -(pxy * np.log(pxy)).sum()
                        mi_vals.append(float(Hx + Hy - Hxy))
                    interlayer_mi_by_dist[d].append(float(np.mean(mi_vals)))

                sids_valid = pile_set_ids_exp_cpu[valid_t.cpu().numpy()]
                for dom in np.unique(sids_valid):
                    mask = (sids_valid == dom)
                    if not np.any(mask): 
                        continue
                    uniq_d, cnt_d = np.unique(routes_tok[mask], axis=0, return_counts=True)
                    for r, c in zip(uniq_d, cnt_d):
                        route_key = tuple(int(x) for x in r)  # tuple ìƒì„±ì€ unique í›„ ì†Œìˆ˜ë§Œ
                        domain_route_counter[reverse_pile_set_map[int(dom)]][route_key] += int(c)
                        total_routes_per_domain[reverse_pile_set_map[int(dom)]] += int(c)

    specialization_results = []
    if run_specialization:
        for layer, expert_data in specialization_stats.items():
            for expert, source_counts in expert_data.items():
                for source, count in source_counts.items():
                    specialization_results.append({
                        "Model": mode, "Layer": layer, "Expert_ID": expert,
                        "Pile_Set_Name": source, "Activation_Count": count
                    })

    confidence_results = []
    confidence_histogram_results = []
    if run_confidence:
        for layer_idx, data in confidence_stats.items():
            if not data['selected_mass']:
                continue
            C = np.asarray(data['selected_mass'], dtype=np.float64)
            D = np.asarray(data['decisiveness'], dtype=np.float64)
            H = np.asarray(data['h_sel'], dtype=np.float64)
            E = np.asarray(data['eec'], dtype=np.float64)
            S1 = np.asarray(data['top1_share'], dtype=np.float64)
            choices = np.asarray(data['expert_choices'], dtype=np.int32)

            if C.size == 0:
                continue

            unique_experts, counts = np.unique(choices, return_counts=True)
            freqs = (counts / counts.sum()).astype(np.float64)
            entropy = float(-(freqs * np.log(freqs + 1e-12)).sum()) if len(freqs) > 0 else 0.0

            # ì„ íƒì§‘í•© ì§ˆëŸ‰ C ê¸°ë°˜ binning (ê³µì • ë¹„êµ)
            hist_counts, bin_edges = np.histogram(C, bins=20, range=(0.0, 1.0))

            confidence_results.append({
                'Model': mode, 'Layer': layer_idx, 'Entropy_Load': entropy,
                'Avg_Selected_Mass': float(C.mean()), 'Std_Selected_Mass': float(C.std()),
                'Avg_Decisiveness': float(D.mean()), 'Avg_Hsel': float(H.mean()),
                'Avg_EEC': float(E.mean()), 'Avg_Top1Share': float(S1.mean()),
                'Total_Decisions': int(C.size)
            })

            for i in range(len(hist_counts)):
                confidence_histogram_results.append({
                    'Model': mode, 'Layer': layer_idx, 'Bin_Index': i,
                    'Bin_Start': float(bin_edges[i]), 'Bin_End': float(bin_edges[i+1]),
                    'Bin_Center': float((bin_edges[i]+bin_edges[i+1])/2),
                    'Count': int(hist_counts[i])
                })

    # Confidence per Source
    source_confidence_results = []
    if run_confidence and confidence_per_source_stats:
        for layer, expert_data in confidence_per_source_stats.items():
            for expert, source_data in expert_data.items():
                for source, Cs in source_data.items():
                    if Cs:
                        arr = np.asarray(Cs, dtype=np.float64)
                        source_confidence_results.append({
                            "Model": mode, "Layer": layer, "Expert_ID": expert,
                            "Pile_Set_Name": source,
                            "Avg_SelectedMass_for_Source": float(arr.mean()),
                            "Std_SelectedMass_for_Source": float(arr.std())
                        })
    if run_specialization and run_confidence and specialization_results and source_confidence_results:
        df_spec = pd.DataFrame(specialization_results)
        df_conf_src = pd.DataFrame(source_confidence_results)
        specialization_results = pd.merge(
            df_spec, df_conf_src,
            on=["Model", "Layer", "Expert_ID", "Pile_Set_Name"], how="left"
        ).to_dict('records')

    # Route summary/detail
    route_summary_rows, route_detail_rows = [], []
    if run_routes and len(domain_route_counter) > 0:
        for dom, cnt in domain_route_counter.items():
            total = total_routes_per_domain[dom]
            if total <= 0: 
                continue
            # ìƒì„¸(ìƒìœ„ 1000)
            for route_tup, c in cnt.most_common(1000):
                route_detail_rows.append({
                    "Model": mode, "Domain": dom,
                    "Route": "-".join(map(str, route_tup)),
                    "Count": int(c), "Share": float(c/total),
                })
            # ìš”ì•½
            counts = np.fromiter(cnt.values(), dtype=np.float64)
            probs = counts / counts.sum()
            route_summary_rows.append({
                "Model": mode, "Domain": dom,
                "RouteConsistency_ModalShare": float(probs.max()),
                "RouteEntropy": float(-(probs * np.log2(probs + 1e-12)).sum()),
                "NumUniqueRoutes": int((counts > 0).sum()),
                "TotalTokens": int(total),
            })

    # Calibration
    for layer_idx, data in confidence_stats.items():
        S1_list = data.get('top1_share', [])
        if not S1_list:
            continue
        probs = np.asarray(S1_list, dtype=np.float64)  # calibration proxy
        calibration_rows.append({
            "Model": mode, "Layer": layer_idx,
            "ECE_S1": float(_ece_top1(probs)), 
            "Brier_S1": float(_brier_top1(probs)),
            "N": int(probs.size)
        })

    # Load-balance
    for layer_idx, counts in load_counts_acc.items():
        if counts.sum() <= 0:
            continue
        gini = _gini(counts)
        cov  = float(counts.std() / (counts.mean() + 1e-12))
        loadbalance_rows.append({
            "Model": mode, "Layer": layer_idx,
            "Gini": float(gini), "CoV": float(cov), "Total": float(counts.sum())
        })

    # Smoothness / Inter-layer MI ì§‘ê³„
    smoothness_rows = []
    if smoothness_vals:
        arr = np.asarray(smoothness_vals, dtype=np.float64)
        smoothness_rows.append({
            "Model": mode, "Smoothness_Mean": float(arr.mean()),
            "Smoothness_Std": float(arr.std()), "Batches": int(arr.size)
        })
    interlayer_mi_rows = []
    for dist, vals in interlayer_mi_by_dist.items():
        arr = np.asarray(vals, dtype=np.float64)
        if arr.size:
            interlayer_mi_rows.append({
                "Model": mode, "LayerDist": int(dist),
                "MI_Mean": float(arr.mean()), "MI_Std": float(arr.std()),
                "Pairs": int(arr.size)
            })

    return (specialization_results, confidence_results, confidence_histogram_results,
            route_summary_rows, route_detail_rows,
            smoothness_rows, interlayer_mi_rows, calibration_rows, loadbalance_rows)

# =====================
# ì „ì²´ ì‹¤í—˜ ì‹¤í–‰
# =====================
def run_mapping_analysis(
    batch_size=44, base_num_experts=16, max_batches=None,
    run_specialization=True, run_confidence=True, run_routes=True
):
    """Expert-to-Source Mapping ë¶„ì„ ì‹¤í–‰"""
    if convert_gpt2_to_moe is None:
        print("âŒ MoE functions could not be imported")
        return

    if not run_specialization and not run_confidence:
        print("â„¹ï¸ No analysis mode selected. Exiting run_mapping_analysis.")
        return

    print("=" * 60)
    print("ğŸ”¬ Expert-to-Source Mapping Analysis")
    print("=" * 60)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    default_config = GPT2Config(
        vocab_size=50257,
        n_positions=1024,
        n_ctx=1024,
        n_embd=1024,
        n_layer=8,
        n_head=8
    )

    # === ë°ì´í„°ì…‹ ë¡œë“œ ë° ì¤€ë¹„ ===
    pile_valid_dataset = load_pile_validation_dataset()

    # 1) ê³ ìœ  pile_set_name ë§¤í•‘
    print("ğŸ“Š Identifying unique data sources from 'meta' column...")
    all_pile_sets = {meta['pile_set_name'] for meta in pile_valid_dataset['meta']}
    source_to_idx = {name: i for i, name in enumerate(sorted(list(all_pile_sets)))}
    print(f"Found {len(source_to_idx)} unique sources: {list(source_to_idx.keys())}")

    # (NEW) ë°ì´í„°ì…‹ì— ì „ì—­ í–‰ ì¸ë±ìŠ¤ ë¶€ì—¬
    pile_valid_dataset = pile_valid_dataset.map(
        lambda ex, idx: {"__row_id__": idx},
        with_indices=True,
        num_proc=os.cpu_count() // 2
    )

    # 2) pile_set_id ì»¬ëŸ¼ ì¶”ê°€
    def add_pile_set_id(example):
        return {'pile_set_id': source_to_idx[example['meta']['pile_set_name']]}

    print("ğŸ”§ Adding 'pile_set_id' column to the dataset...")
    pile_valid_dataset = pile_valid_dataset.map(add_pile_set_id, num_proc=os.cpu_count() // 2)

    # 3) DataLoader (ì»¬ëŸ¼ì— '__row_id__' í¬í•¨!)
    pile_valid_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "pile_set_id", "__row_id__"])
    pile_valid_loader = DataLoader(
        pile_valid_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=os.cpu_count() // 2,
        pin_memory=True,
        prefetch_factor=4,
        persistent_workers=True
    )

    # ê¸°ë³¸: ì „ì²´ì˜ 10%ë§Œ ì‚¬ìš© (ëª…ì‹œì  max_batches ì§€ì • ì‹œ ì „ì²´/ì§€ì • ë¹„ìœ¨ë¡œ)
    if max_batches is None:
        num_batches = len(pile_valid_loader)
        limit_batches = max(1, int(num_batches * 0.1))
        print(f"ğŸ“Š ì „ì²´ {num_batches}ê°œ ë°°ì¹˜ ì¤‘ ì•½ 10%ì¸ {limit_batches}ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        max_batches = limit_batches
    elif max_batches == "debug":
        # Debug ëª¨ë“œ: ì „ì²´ì˜ 0.1%ë§Œ ì‚¬ìš©
        num_batches = len(pile_valid_loader)
        limit_batches = max(1, int(num_batches * 0.001))
        print(f"ğŸ› DEBUG ëª¨ë“œ: ì „ì²´ {num_batches}ê°œ ë°°ì¹˜ ì¤‘ ì•½ 0.1%ì¸ {limit_batches}ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        max_batches = limit_batches

    potential_model_paths = {
        "hash":       os.path.join(CHECKPOINTS_DIR, "hash_exp1", "best_checkpoint.safetensors"),
        "ours_com":   os.path.join(CHECKPOINTS_DIR, "ours_com_exp1", "best_checkpoint.safetensors"),
        "gshard":     os.path.join(CHECKPOINTS_DIR, "gshard_exp1", "best_checkpoint.safetensors"),
        "switch":     os.path.join(CHECKPOINTS_DIR, "switch_exp1", "best_checkpoint.safetensors"),
        "stablemoe":  os.path.join(CHECKPOINTS_DIR, "stablemoe_exp1", "best_checkpoint.safetensors"),
        "hypermoe":   os.path.join(CHECKPOINTS_DIR, "hypermoe_exp1", "best_checkpoint.safetensors"),
        "xmoe":       os.path.join(CHECKPOINTS_DIR, "xmoe_exp1", "best_checkpoint.safetensors"),
        "expert_choice": os.path.join(CHECKPOINTS_DIR, "expert_choice_exp1", "best_checkpoint.safetensors"),
    }

    available_model_paths = {
        mode: path for mode, path in potential_model_paths.items() if os.path.exists(path)
    }

    if not available_model_paths:
        print("âŒ No model checkpoints found")
        return

    print(f"\nğŸ“‚ Found {len(available_model_paths)} models:")
    for mode in available_model_paths:
        print(f"   âœ“ {mode}")

    # === ë¶„ì„ ===
    all_specialization_results = []
    all_confidence_results = []
    all_histogram_results = []
    all_route_summary = []
    all_route_detail = []
    all_smoothness = []
    all_interlayer_mi = []
    all_calibration = []
    all_loadbalance = []

    for mode, ckpt_path in available_model_paths.items():
        print(f"\n{'=' * 60}")
        print(f"ğŸ“Š Analyzing: {mode}")
        print(f"{'=' * 60}")

        # Config
        config_dir = os.path.dirname(ckpt_path)
        config_path = os.path.join(config_dir, "config.json")

        if os.path.exists(config_path):
            config = GPT2Config.from_pretrained(config_dir)
        else:
            config = default_config

        # ëª¨ë¸ ì´ˆê¸°í™”
        model = GPT2LMHeadModel(config)

        # MoE ë³€í™˜
        if mode != "dense":
            freq_dict = None
            current_num_experts = base_num_experts
            if mode == "ours_com":
                current_num_experts += 1  # base(16) + 1 global = 17

            if mode == "hash":
                if not os.path.exists(HASH_TABLE_PATH):
                    print(f"âš ï¸ Hash table not found: {HASH_TABLE_PATH}")
                    continue
                freq_dict = {'__load_from_file__': HASH_TABLE_PATH}

            model = convert_gpt2_to_moe(
                model, config,
                mode=mode,
                num_experts=current_num_experts,
                alpha=0.01,
                capacity_factor=1.25,
                freq_dict=freq_dict
            )

        if mode == "hash":
            patch_model_for_hash_moe(model)
        elif mode == "ours_com":
            patch_model_for_ours_com(model)
        elif mode != "dense":
            patch_model_basic(model)

        if mode == "ours_com":
            root_router = model.transformer.h[0].mlp.moe.shared_router
            for l in range(1, config.n_layer):
                model.transformer.h[l].mlp.moe.shared_router = root_router

        try:
            load_safetensors(model, ckpt_path, mode=mode, strict=False)
        except Exception as e:
            print(f"âŒ Failed to load {mode}: {e}")
            continue

        model.to(device)

        model.lm_head.weight = model.transformer.wte.weight
        if not torch.equal(model.transformer.wte.weight.data, model.lm_head.weight.data):
            print("âš ï¸ Weight tying broken after GPU move, restoring...")
            model.transformer.wte.weight = model.lm_head.weight
            print("âœ… Weight tying restored after GPU move!")

        (spec_list, conf_list, hist_list, route_sum, route_det,
            smooth_rows, mi_rows, calib_rows, lb_rows) = analyze_expert_specialization(
                model=model,
                dataloader=pile_valid_loader,
                device=device,
                mode=mode,
                pile_set_map=source_to_idx,
                max_batches=max_batches,
                run_specialization=run_specialization,
                run_confidence=run_confidence,
                run_routes=run_routes,
            )

        if run_specialization:
            if not spec_list:
                print(f"âš ï¸ {mode}: No expert selections were recorded!")
            else:
                df_specialization = pd.DataFrame(spec_list)
                df_specialization['Model'] = mode
                all_specialization_results.append(df_specialization)

        if run_confidence and len(conf_list) > 0:
            df_confidence = pd.DataFrame(conf_list)
            df_confidence['Model'] = mode
            all_confidence_results.append(df_confidence)

        if run_confidence and hist_list:
            df_histogram = pd.DataFrame(hist_list)
            df_histogram['Model'] = mode
            all_histogram_results.append(df_histogram)

        if route_sum:
            all_route_summary.extend(route_sum)
        if route_det:
            all_route_detail.extend(route_det)

        if smooth_rows:      all_smoothness.extend(smooth_rows)
        if mi_rows:          all_interlayer_mi.extend(mi_rows)
        if calib_rows:       all_calibration.extend(calib_rows)
        if lb_rows:          all_loadbalance.extend(lb_rows)

        print(f"âœ… {mode} analysis complete")
        if run_specialization and spec_list:
            print(f"   ğŸ“Š Specialization: {len(df_specialization)} combinations, {df_specialization['Activation_Count'].sum():,} selections")
        if run_confidence and len(conf_list) > 0:
            print(f"   ğŸ¯ Confidence: {len(df_confidence)} layers analyzed, "
                  f"avg entropy {df_confidence['Entropy_Load'].mean():.3f}")

        del model
        torch.cuda.empty_cache()

    if all_specialization_results or all_confidence_results:
        output_dir = CHECKPOINTS_DIR
        os.makedirs(output_dir, exist_ok=True)

        if run_specialization and all_specialization_results:
            combined_specialization_df = pd.concat(all_specialization_results, ignore_index=True)

            print("\n\nğŸ“Š Calculating Global Source Distribution from analyzed batches...")
            total_activations = combined_specialization_df['Activation_Count'].sum()
            global_source_counts = combined_specialization_df.groupby('Pile_Set_Name')['Activation_Count'].sum()
            global_source_distribution = global_source_counts / total_activations
            print(f"âœ… Total tokens analyzed across all models: {total_activations:,}")

            print("\nğŸ”¬ Calculating Specialization Indices...")
            combined_specialization_df['Expert_Total_Activation'] = combined_specialization_df.groupby(
                ['Model', 'Layer', 'Expert_ID']
            )['Activation_Count'].transform('sum')
            combined_specialization_df['P_Source_Given_Expert'] = (
                combined_specialization_df['Activation_Count'] / combined_specialization_df['Expert_Total_Activation']
            )
            global_df = global_source_distribution.rename("P_Source_Global").reset_index()
            combined_specialization_df = pd.merge(combined_specialization_df, global_df, on='Pile_Set_Name', how='left')

            epsilon = 1e-10
            combined_specialization_df['Specialization_Index'] = (
                combined_specialization_df['P_Source_Given_Expert'] / (combined_specialization_df['P_Source_Global'] + epsilon)
            )

            max_spec_idx = combined_specialization_df.groupby(
                ['Model', 'Layer', 'Expert_ID']
            )['Specialization_Index'].idxmax()
            max_specialization_per_expert = combined_specialization_df.loc[max_spec_idx,
                ['Model', 'Layer', 'Expert_ID', 'Pile_Set_Name', 'Specialization_Index']
            ].rename(columns={
                'Pile_Set_Name': 'Max_Specialized_Source',
                'Specialization_Index': 'Max_Specialization_Index'
            })
            combined_specialization_df = pd.merge(
                combined_specialization_df,
                max_specialization_per_expert,
                on=['Model', 'Layer', 'Expert_ID'],
                how='left'
            )

            print("\nğŸ”¬ Calculating Source Entropy for each expert...")
            expert_source_distribution = combined_specialization_df.groupby(
                ['Model', 'Layer', 'Expert_ID']
            )['P_Source_Given_Expert'].apply(list)
            expert_focus = expert_source_distribution.apply(lambda dist: shannon_entropy(dist, base=2)).reset_index()
            expert_focus = expert_focus.rename(columns={'P_Source_Given_Expert': 'Source_Entropy'})
            combined_specialization_df = pd.merge(
                combined_specialization_df, expert_focus,
                on=['Model', 'Layer', 'Expert_ID'],
                how='left'
            )

            ideal_final_columns = [
                'Model', 'Layer', 'Expert_ID', 'Pile_Set_Name',
                'Activation_Count', 'Expert_Total_Activation',
                'P_Source_Global', 'P_Source_Given_Expert', 'Specialization_Index',
                'Max_Specialized_Source', 'Max_Specialization_Index',
                'Source_Entropy',
                # â–¼ ìƒˆ ì»¬ëŸ¼ë“¤
                'Avg_SelectedMass_for_Source', 'Std_SelectedMass_for_Source'
            ]
            final_columns = [col for col in ideal_final_columns if col in combined_specialization_df.columns]
            combined_specialization_df = combined_specialization_df[final_columns]

            specialization_output_path = os.path.join(output_dir, "expert_source_mapping_with_specialization.csv")
            combined_specialization_df.to_csv(specialization_output_path, index=False)

            print(f"\n{'=' * 60}")
            print(f"âœ… Specialization Results saved to:")
            print(f"   {specialization_output_path}")
            print(f"{'=' * 60}")

            print(f"\nğŸ“ˆ Top 10 Expert-Source pairs by Specialization Index:")
            top_specialized = combined_specialization_df.nlargest(10, 'Specialization_Index')[
                ['Model', 'Layer', 'Expert_ID', 'Pile_Set_Name',
                 'Specialization_Index', 'P_Source_Given_Expert', 'P_Source_Global']
            ]
            print(top_specialized.to_string(index=False))

            print(f"\nğŸ“Š Specialization Statistics by Model:")
            for m in combined_specialization_df['Model'].unique():
                mode_df = combined_specialization_df[combined_specialization_df['Model'] == m]
                total_selections = mode_df['Activation_Count'].sum()
                avg_specialization = mode_df.groupby(
                    ['Layer', 'Expert_ID']
                )['Specialization_Index'].max().mean()
                print(f"\n  {m}:")
                print(f"    Total selections: {total_selections:,}")
                print(f"    Unique (Layer, Expert, Source) combinations: {len(mode_df)}")
                print(f"    Avg. max specialization index per expert: {avg_specialization:.3f}")

        elif run_specialization:
            print("\nâš ï¸ Specialization analysis failed or no data collected.")

        if run_confidence and all_confidence_results:
            combined_confidence_df = pd.concat(all_confidence_results, ignore_index=True)
            confidence_output_path = os.path.join(output_dir, "routing_confidence_analysis.csv")
            combined_confidence_df.to_csv(confidence_output_path, index=False)

            print(f"\n{'=' * 60}")
            print(f"âœ… Confidence Analysis Results saved to:")
            print(f"   {confidence_output_path}")
            print(f"{'=' * 60}")

            print(f"Total records: {len(combined_confidence_df)}")

            print(f"\nğŸ¯ Routing Confidence Statistics by Model:")
            print(f"\n{'Model':<20} {'Entropy_Load':>12} {'Avg_C':>10} {'Avg_D':>10} {'Avg_Hsel':>10} {'Avg_EEC':>10} {'Avg_S1':>10}")
            print("-" * 80)
            for m in combined_confidence_df['Model'].unique():
                mode_conf_df = combined_confidence_df[combined_confidence_df['Model'] == m]
                print(f"{m:<20} "
                      f"{mode_conf_df['Entropy_Load'].mean():>12.4f} "
                      f"{mode_conf_df['Avg_Selected_Mass'].mean():>10.4f} "
                      f"{mode_conf_df['Avg_Decisiveness'].mean():>10.4f} "
                      f"{mode_conf_df['Avg_Hsel'].mean():>10.4f} "
                      f"{mode_conf_df['Avg_EEC'].mean():>10.4f} "
                      f"{mode_conf_df['Avg_Top1Share'].mean():>10.4f}")

            print("\nğŸ’¡ Interpretation Guide (Selected-Mass distributions):")
            print("  â€¢ Entropy_Load: ì „ë¬¸ê°€ ì‚¬ìš© ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ (ë‚®ì„ìˆ˜ë¡ ì§‘ì¤‘)")
            print("  â€¢ Avg_C: í‰ê·  Selected-Mass (ì„ íƒì§‘í•© í™•ë¥  í•©)")
            print("  â€¢ Avg_D: í‰ê·  Decisiveness (top-1ê³¼ top-2 ì°¨ì´/í•©)")
            print("  â€¢ Avg_Hsel: í‰ê·  ì„ íƒì§‘í•© ë‚´ë¶€ ì—”íŠ¸ë¡œí”¼")
            print("  â€¢ Avg_EEC: í‰ê·  Effective Expert Count")
            print("  â€¢ Avg_S1: í‰ê·  Top-1 ì ìœ ìœ¨ (ì„ íƒì§‘í•© ë‚´)")

        if run_confidence and all_histogram_results:
            combined_histogram_df = pd.concat(all_histogram_results, ignore_index=True)
            histogram_output_path = os.path.join(output_dir, "routing_confidence_histogram.csv")
            combined_histogram_df.to_csv(histogram_output_path, index=False)

            print(f"\n{'=' * 60}")
            print(f"âœ… Confidence Histogram Results saved to:")
            print(f"   {histogram_output_path}")
            print(f"{'=' * 60}")

            print(f"Total histogram records: {len(combined_histogram_df)}")
            print("ğŸ’¡ Use this for plotting confidence distributions.")

        elif run_confidence:
            print("\nâš ï¸ Confidence histogram analysis failed or no data collected.")

        if all_route_detail:
            pd.DataFrame(all_route_detail).to_csv(
                os.path.join(output_dir, "route_sequences_detail.csv"), index=False
            )
            print(f"\nâœ… Route sequence details saved to {os.path.join(output_dir, 'route_sequences_detail.csv')}")

        if all_route_summary:
            df_rs = pd.DataFrame(all_route_summary)
            df_rs.to_csv(os.path.join(output_dir, "route_sequences_summary.csv"), index=False)
            print(f"âœ… Route sequence summary saved to {os.path.join(output_dir, 'route_sequences_summary.csv')}")

            disp = (df_rs.sort_values(["RouteConsistency_ModalShare"], ascending=False)
                        .groupby("Model").head(5))
            print("\nğŸ Route Consistency (top domains by modal share):")
            print(disp.to_string(index=False))

        if all_smoothness:
            pd.DataFrame(all_smoothness).to_csv(
                os.path.join(output_dir, "route_smoothness.csv"), index=False
            )
            print("âœ… Saved route_smoothness.csv")

        if all_interlayer_mi:
            pd.DataFrame(all_interlayer_mi).to_csv(
                os.path.join(output_dir, "interlayer_mi.csv"), index=False
            )
            print("âœ… Saved interlayer_mi.csv")

        if all_calibration:
            pd.DataFrame(all_calibration).to_csv(
                os.path.join(output_dir, "routing_calibration.csv"), index=False
            )
            print("âœ… Saved routing_calibration.csv")
            print("   Columns now use S1 (Top-1 share within selected set) as the probability proxy.")

        if all_loadbalance:
            pd.DataFrame(all_loadbalance).to_csv(
                os.path.join(output_dir, "load_balance.csv"), index=False
            )
            print("âœ… Saved load_balance.csv")

    else:
        print("âŒ No results collected or no analysis mode selected.")


if __name__ == "__main__":
    BASE_EXPERTS_COUNT = 16
    EVAL_BATCH_SIZE = 44

    run_mapping_analysis(
        batch_size=EVAL_BATCH_SIZE,
        base_num_experts=BASE_EXPERTS_COUNT,
        max_batches=None,              # Noneì´ë©´ ì „ì²´ì˜ 10%ë§Œ ì‚¬ìš©
        run_specialization=True,       # ì „ë¬¸ê°€-ì†ŒìŠ¤ ë§¤í•‘ ë¶„ì„
        run_confidence=True,           # ë¼ìš°íŒ… ì‹ ë¢°ë„ ë¶„ì„
        run_routes=True,               # ë¼ìš°íŠ¸ ì‹œí€€ìŠ¤ ë¶„ì„
    )